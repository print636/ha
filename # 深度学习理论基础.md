# 深度学习理论基础

## 1.

### 数据集
用于喂给ai训练学习的数据的集合

### 标签
数据样本所对应的标签，也就是样本的正确答案，如要输出样本的类别，类别就是标签

### 特征
数据样本的自变量，输入后根据特征来预测或分类

## 2.

### 损失函数
表示神经网络性能的恶劣程度，最有名的损失函数是均方误差，类似与高中概率题中的独立性检验

### 作用
比较不同参数时损失函数的总和来寻找一个使其值最小的参数，为什么不直接把识别精度作为标准呢，因为在一个微小的权重参数改变时识别精度可能不变，例如计算特定人个数时无法存在小数个人（无法连续），也就无法判断，此时损失函数的导数便可以反应结果。

## 3.

### softmax
softmax函数是一个将数转换为概率分布的函数，常用于多分类问题的输出层。

## 4.

### MLP
MLP是多层感知器，由输入层，输出层，一或多个隐藏层组成。

## 5.

### 激活函数
将输入信号转换为输出信号，可以将线性转换为非线性，对神经网络学习有重要意义。常用的有sigmoid, ReLU等。

## 6.

### 过拟合
机器只能拟合训练数据，但不能很好拟合不包含训练数据的其他数据的状态

### 减少方法
1.权值衰减，对大的权重进行惩罚
2.Dropout,随机删除神经元

## 7.

### 超参数
神经元数量，学习率，权值衰减，一批数据的大小

## 8.

### 卷积层
是CNN中的一个关键层，用于提取输入数据的局部特征

### 计算方式
填充或不填充，如4*4大小的数据应用3*3的滤波器后输出2*2大小的数据，而应用幅度为1的填充后会输出4*4大小的数据，此时可以在保持空间大小不变的情况下将数据传给下一层。

## 9.

### 监督学习
训练数据带有标签，模型可以通过学习从输入到已知标签的映射来训练。

### 无监督学习
训练数据无标签，模型直接从数据中找出隐藏的结构或关系。

## 10.

### 经典模型

CNN:处理具有类似网络结构的神经网络

RNN:用于处理序列数据的神经网络，LSTM是其的变体，能记住长期信息，解决长期依赖问题。

GNN:处理图结构的神经网络，如预测两个节点之间的关系。

attention机制:一种权重分配机制，使模型聚焦于关键部分

transformer:一个基于attention机制的模型架构，被广泛应用于NLP任务。

## 11.

### 相关概念
NLP:自然语言处理，是计算机科学领域与人工智能领域中的一个方向，专注于人与计算机之间如何有效、准确地使用自然语言进行通信。

CV:计算机视觉，是研究如何让计算机从图像或视频中获取信息、理解内容并作出决策的科学领域







